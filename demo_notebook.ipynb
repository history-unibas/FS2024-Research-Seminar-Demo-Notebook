{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einleitung\n",
    "Dieses Jupyter Notebook demonstriert, wie aufbereitete Daten des Historischen Grundbuchs der Stadt Basel durchsucht werden können.\n",
    "\n",
    "Bemerkung: Je nach Suche, Suchbegriff und Suchzeitraum kann die Suche einige Zeit dauern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importiere Packages\n",
    "Installiere für dieses Notebook notwendige Packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "FILENAME_REQUIREMENTS = 'requirements.txt'\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    in_colab = True\n",
    "except ImportError:\n",
    "    in_colab = False\n",
    "\n",
    "if in_colab:\n",
    "\n",
    "    # Install the package 'thefuzz'.\n",
    "    subprocess.run(['pip', 'install', 'thefuzz'], check=True)\n",
    "\n",
    "else:\n",
    "    \n",
    "    # Install all packages listed in local available requirements.txt.\n",
    "    subprocess.run(['pip', 'install', '-r', FILENAME_REQUIREMENTS], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importiere für dieses Skript notwendige Funktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from pyproj import Transformer\n",
    "from ipyleaflet import Map, WMSLayer, Circle, Popup\n",
    "import ipywidgets as widgets\n",
    "from thefuzz import fuzz, process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definiere Arbeitsverzeichnis für Google Colab\n",
    "Definiere Arbeitsverzeichnis in Google Colab zur Speicherung Daten und Ergebnisse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if in_colab:\n",
    "  # Define the working directory in google colab.\n",
    "  WORKING_DIRECTORY = '/content/drive/My Drive/Colab Notebooks/'\n",
    "\n",
    "  # Mount personal google drive.\n",
    "  drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bereite Datengrundlage auf\n",
    "Lade und entpacke für die Suche verwendete Datengrundlage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME_DATA = 'hgb_corpus_24_07_26_inline_full_updated.zip'\n",
    "URL_DATA = 'https://github.com/history-unibas/FS2024-Research-Seminar-Demo-Notebook/raw/main/hgb_corpus_24_07_26_inline_full_updated.zip'\n",
    "\n",
    "if in_colab:\n",
    "    \n",
    "    if not os.path.exists(WORKING_DIRECTORY + FILENAME_DATA):\n",
    "      \n",
    "      # Download the data in working directory.\n",
    "      subprocess.run(['wget', URL_DATA, '-O', WORKING_DIRECTORY + FILENAME_DATA], check=True)\n",
    "\n",
    "      # Unzip the file.\n",
    "      with zipfile.ZipFile(WORKING_DIRECTORY + FILENAME_DATA, 'r') as myzip:\n",
    "          myzip.extractall(WORKING_DIRECTORY)\n",
    "      \n",
    "    # Update the filepath.\n",
    "    FILENAME_DATA = WORKING_DIRECTORY + os.path.splitext(FILENAME_DATA)[0] + '.xml'\n",
    "\n",
    "else:\n",
    "\n",
    "    # Unzip the file.\n",
    "    with zipfile.ZipFile(FILENAME_DATA, 'r') as myzip:\n",
    "        myzip.extractall('.')\n",
    "\n",
    "    # Update the filepath.\n",
    "    FILENAME_DATA = os.path.splitext(FILENAME_DATA)[0] + '.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suchen\n",
    "In diesem Jupyter Notebook kann auf folgende drei Arten gesucht werden:\n",
    "- Freitextsuche: Suche nach einem beliebigem Begriff in Paragraphen von Dokumenten.\n",
    "- Entitätssuche: Suche nach annotierten Entitäten in Paragraphen von Dokumenten.\n",
    "- Eventsuche: Suche nach annotierten Events in Paragraphen von Dokumenten.\n",
    "\n",
    "Bei jeder Suche wird einen Suchzeitraum definiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freitextsuche\n",
    "Beispiel: Suche nach dem Begriff 'wysung' in Paragraphen von Dokumenten im Zeitraum 1550 – 1600."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiere Suchparameter\n",
    "Festlegen der gewünschten Parameter für die Suche.\n",
    "\n",
    "Die 'Fuzzy'-Suche ist eine fehlertolerante Suche.\n",
    "\n",
    "Mit dem 'Verhältnis-Schwellenwert' wird definiert, wie ähnlich Suchresultate zum Suchbegriff sein sollen. Je kleiner dieser Wert, je mehr Treffer werden generiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set search word.\n",
    "SEARCH_KEYWORD = 'wysung'\n",
    "\n",
    "# Define search period.\n",
    "YEAR_MIN = 1550\n",
    "YEAR_MAX = 1600\n",
    "\n",
    "# Define whether to search for the exact search term.\n",
    "DO_EXACT_SEARCH = False\n",
    "\n",
    "# Ratio threshold between 0 and 100 for inexact search.\n",
    "RATIO_THRESHOLD = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suche ausführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load xml file.\n",
    "context = ET.iterparse(FILENAME_DATA, events=('start', 'end'))\n",
    "\n",
    "# Create an empty dataframe to store the search results.\n",
    "results = pd.DataFrame(\n",
    "    columns=['id', 'house', 'coord_x', 'coord_y',\n",
    "             'pages', 'year', 'text', 'imagelinks',\n",
    "             'keyword', 'ratio'\n",
    "             ]\n",
    "    )\n",
    "\n",
    "# Iterate over each element.\n",
    "collection_element = None\n",
    "for event, elem in context:\n",
    "\n",
    "    # The start event is triggered when the parser encounters the opening tag of an element.\n",
    "    if event == 'start' and elem.tag == 'Collection':\n",
    "            \n",
    "        # Store current collection element.\n",
    "        collection_element = elem\n",
    "    \n",
    "    # The end event is triggered when the parser encounters the closing tag of an element.\n",
    "    elif event == 'end' and elem.tag == 'Document':\n",
    "            \n",
    "        # Get the header element.\n",
    "        header = elem.find('Header')\n",
    "        \n",
    "        # Determine the year of the document.\n",
    "        year = int(header.get('year'))\n",
    "\n",
    "        # Skip the document if not in desired search period.\n",
    "        if year < YEAR_MIN or year > YEAR_MAX:\n",
    "            continue\n",
    "\n",
    "        # Get the text attribute.\n",
    "        text = header.get('text')\n",
    "\n",
    "        if DO_EXACT_SEARCH:\n",
    "            \n",
    "            # Continue if keyword not in text.\n",
    "            if SEARCH_KEYWORD not in text:\n",
    "                continue\n",
    "            \n",
    "            else:\n",
    "                keyword = SEARCH_KEYWORD\n",
    "                ratio = None\n",
    "        \n",
    "        else:\n",
    "            # Perform a fuzzy search.\n",
    "            keyword_ratio = process.extractOne(\n",
    "                SEARCH_KEYWORD, text.split(),\n",
    "                score_cutoff=RATIO_THRESHOLD,\n",
    "                scorer=fuzz.ratio\n",
    "                )\n",
    "\n",
    "            # Continue if ratio is smaller than threshold.\n",
    "            if keyword_ratio is None:\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                keyword, ratio = keyword_ratio\n",
    "        \n",
    "        # Store search result in dataframe.\n",
    "        results.loc[len(results)] = [\n",
    "            collection_element.attrib['id'],\n",
    "            collection_element.attrib['house'],\n",
    "            collection_element.attrib['coord_x'],\n",
    "            collection_element.attrib['coord_y'],\n",
    "            header.get('pages'),\n",
    "            year,\n",
    "            text,\n",
    "            header.get('imagelinks'),\n",
    "            keyword,\n",
    "            ratio\n",
    "            ]\n",
    "\n",
    "# Print the number of search results.\n",
    "print(f'The keyword \"{SEARCH_KEYWORD}\" was found in {results.shape[0]} documents '\n",
    "      f'in the period between {YEAR_MIN} and {YEAR_MAX}.'\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entitätssuche\n",
    "Beispiel: Suche nach dem Begriff \"Fischer\" in Paragraphen von Dokumenten im Zeitraum 1550 – 1600."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suchparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set search word.\n",
    "SEARCH_KEYWORD = 'Fischer'\n",
    "\n",
    "# Define the mention subtype.\n",
    "MENTION_SUBTYPE = 'occ'\n",
    "\n",
    "# Define search period.\n",
    "YEAR_MIN = 1550\n",
    "YEAR_MAX = 1600\n",
    "\n",
    "# Define whether to search for the exact search term.\n",
    "DO_EXACT_SEARCH = False\n",
    "\n",
    "# Ratio threshold between 0 and 100 for inexact search.\n",
    "RATIO_THRESHOLD = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suche ausführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load xml file.\n",
    "context = ET.iterparse(FILENAME_DATA, events=('start', 'end'))\n",
    "\n",
    "# Create an empty dataframe to store the search results.\n",
    "results = pd.DataFrame(\n",
    "    columns=['id', 'house', 'coord_x', 'coord_y',\n",
    "             'pages', 'year', 'text', 'imagelinks',\n",
    "             'keyword', 'ratio',\n",
    "             'head', 'confidence'\n",
    "             ]\n",
    "    )\n",
    "\n",
    "# Iterate over each element.\n",
    "collection_element = None\n",
    "for event, elem in context:\n",
    "    if event == 'start' and elem.tag == 'Collection':\n",
    "            \n",
    "        # Store current collection element.\n",
    "        collection_element = elem\n",
    "    \n",
    "    elif event == 'end' and elem.tag == 'Document':\n",
    "            \n",
    "        # Get the header element.\n",
    "        header = elem.find('Header')\n",
    "        \n",
    "        # Determine the year of the document.\n",
    "        year = int(header.get('year'))\n",
    "\n",
    "        # Skip the document if not in desired search period.\n",
    "        if year < YEAR_MIN or year > YEAR_MAX:\n",
    "            continue\n",
    "        \n",
    "        # Iterate over all attributes of the document.\n",
    "        for attribute in elem.findall(f'./Body//Attribute[@mention_subtype=\"{MENTION_SUBTYPE}\"]'):\n",
    "\n",
    "            # Get the header text.\n",
    "            head = attribute.find('.//Head')\n",
    "            head_text = head.text\n",
    "\n",
    "            # Skip attribute with no header text.\n",
    "            if not head_text:\n",
    "                continue\n",
    "            \n",
    "            if DO_EXACT_SEARCH:\n",
    "                \n",
    "                # Continue if keyword not in text.\n",
    "                if SEARCH_KEYWORD not in head_text:\n",
    "                    continue\n",
    "                \n",
    "                else:\n",
    "                    keyword = SEARCH_KEYWORD\n",
    "                    ratio = None\n",
    "            \n",
    "            else:\n",
    "                # Perform a fuzzy search.\n",
    "                keyword_ratio = process.extractOne(\n",
    "                    SEARCH_KEYWORD, head_text.split(),\n",
    "                    score_cutoff=RATIO_THRESHOLD,\n",
    "                    scorer=fuzz.ratio\n",
    "                    )\n",
    "\n",
    "                # Continue if ratio is smaller than threshold.\n",
    "                if keyword_ratio is None:\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    keyword, ratio = keyword_ratio\n",
    "\n",
    "            # Store search result in dataframe.\n",
    "            results.loc[len(results)] = [\n",
    "                collection_element.attrib['id'],\n",
    "                collection_element.attrib['house'],\n",
    "                collection_element.attrib['coord_x'],\n",
    "                collection_element.attrib['coord_y'],\n",
    "                header.get('pages'),\n",
    "                year,\n",
    "                header.get('text'),\n",
    "                header.get('imagelinks'),\n",
    "                keyword,\n",
    "                ratio,\n",
    "                head_text,\n",
    "                attribute.attrib.get('confidence')\n",
    "                ]\n",
    "\n",
    "# Print the number of search results.\n",
    "print(f'The keyword \"{SEARCH_KEYWORD}\" was found in {results.shape[0]} entities of '\n",
    "      f'entity mention subtype \"{MENTION_SUBTYPE}\" in documents from the period {YEAR_MIN} to {YEAR_MAX}.'\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eventsuche\n",
    "Beispiel: Suche nach Fröhnungsdokumenten in Dokumenten im Zeitraum 1550 – 1600."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suchparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the event type.\n",
    "EVENT_TYPE = 'seizure'\n",
    "\n",
    "# Define search period.\n",
    "YEAR_MIN = 1550\n",
    "YEAR_MAX = 1600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suche ausführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load xml file.\n",
    "context = ET.iterparse(FILENAME_DATA, events=('start', 'end'))\n",
    "\n",
    "# Create an empty dataframe to store the search results.\n",
    "results = pd.DataFrame(\n",
    "    columns=['id', 'house', 'coord_x', 'coord_y',\n",
    "             'pages', 'year', 'text', 'imagelinks',\n",
    "             'keyword',\n",
    "             'trigger_text', 'confidence'\n",
    "             ]\n",
    "    )\n",
    "\n",
    "# Iterate over each element.\n",
    "collection_element = None\n",
    "for event, elem in context:\n",
    "    if event == 'start' and elem.tag == 'Collection':\n",
    "            \n",
    "        # Store current collection element.\n",
    "        collection_element = elem\n",
    "    \n",
    "    elif event == 'end' and elem.tag == 'Document':\n",
    "            \n",
    "        # Get the header element.\n",
    "        header = elem.find('Header')\n",
    "        \n",
    "        # Determine the year of the document.\n",
    "        year = int(header.get('year'))\n",
    "\n",
    "        # Skip the document if not in desired search period.\n",
    "        if year < YEAR_MIN or year > YEAR_MAX:\n",
    "            continue\n",
    "        \n",
    "        # Iterate over all events of the document.\n",
    "        for doc_event in elem.findall(f'./Standoff/Events//Event[@type=\"{EVENT_TYPE}\"]'):\n",
    "\n",
    "            # Get the trigger text attribute.\n",
    "            trigger = doc_event.find('.//Trigger')\n",
    "            trigger_text = trigger.attrib.get('text')\n",
    "            \n",
    "            # Store selected elements in dataframe.\n",
    "            results.loc[len(results)] = [\n",
    "                collection_element.attrib['id'],\n",
    "                collection_element.attrib['house'],\n",
    "                collection_element.attrib['coord_x'],\n",
    "                collection_element.attrib['coord_y'],\n",
    "                header.get('pages'),\n",
    "                year,\n",
    "                header.get('text'),\n",
    "                header.get('imagelinks'),\n",
    "                trigger_text,\n",
    "                trigger_text,\n",
    "                trigger.attrib.get('confidence')\n",
    "                ]\n",
    "\n",
    "# Print the number of search results.\n",
    "print(f'The event type \"{EVENT_TYPE}\" was found in {results.shape[0]} events '\n",
    "      f'in documents from the period {YEAR_MIN} to {YEAR_MAX}.'\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ergebnisse in Tabelle darstellen\n",
    "Die Suchergebnisse werden in einer Tabelle dargestellt. Der Suchbegriff wird im Text vorgehoben und das zugehörige Digitalisat zur Verfügung gestellt.\n",
    "\n",
    "In diesem Notebook werden maximal vier Resultate dargestellt. Alle Suchresultate werden in einer HTML-Datei im Arbeitsverzeichnis gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filename for exported html table.\n",
    "FILENAME_HTML = 'search_results.html'\n",
    "\n",
    "# Reduce the table of results for the display. \n",
    "filtered_results = results[['id', 'house', 'pages', 'year', 'text', 'imagelinks', 'keyword']].copy()\n",
    "\n",
    "# Define the integration of the images in the table.\n",
    "def make_clickable_images(val):\n",
    "    urls = val.split(\" | \")\n",
    "    img_tags = [f'<a href=\"{url}\" target=\"_blank\"><img src=\"{url}\" width=\"100\" /></a>' for url in urls]\n",
    "    return \" \".join(img_tags)\n",
    "\n",
    "filtered_results['imagelinks'] = filtered_results['imagelinks'].apply(make_clickable_images)\n",
    "\n",
    "# Highlight the search word.\n",
    "def highlight_keyword(keyword, val):\n",
    "    if keyword in val:\n",
    "        return val.replace(f'{keyword}', f'<span style=\"color: red;\">{keyword}</span>')\n",
    "    return val\n",
    "\n",
    "filtered_results['text'] = filtered_results.apply(\n",
    "    lambda row: highlight_keyword(row['keyword'], row['text']), axis=1\n",
    ")\n",
    "filtered_results.drop(columns=['keyword'], inplace=True)\n",
    "\n",
    "# Clean up page breaks.\n",
    "def clean_pagebreaks(val):\n",
    "    return val.replace('\\n', '<br>')\n",
    "\n",
    "filtered_results['text'] = filtered_results['text'].apply(\n",
    "    lambda x: clean_pagebreaks(x)\n",
    "    )\n",
    "\n",
    "# Render the dataframe as HTML.\n",
    "html = filtered_results.to_html(escape=False)\n",
    "\n",
    "# Export all search results as html file.\n",
    "if in_colab:\n",
    "    with open(WORKING_DIRECTORY + FILENAME_HTML, 'w', encoding='utf-8') as file:\n",
    "        file.write(html)\n",
    "else:\n",
    "    with open(FILENAME_HTML, 'w', encoding='utf-8') as file:\n",
    "        file.write(html)\n",
    "\n",
    "# Display a maximum of four results in a table.\n",
    "display(HTML(filtered_results.to_html(escape=False, max_rows=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogramm der Resultate über die Zeit\n",
    "Die Verteilung der Suchresultate über den Suchzeitraum wird in einem Histogramm visualisiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the appearance of the histogram.\n",
    "plt.figure(figsize=(10, 6))\n",
    "n, bins, patches = plt.hist(\n",
    "    results['year'],\n",
    "    bins=range(min(results['year']), max(results['year']), 1),\n",
    "    edgecolor='black'\n",
    "    )\n",
    "plt.title('Histogram')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.gca().set_axisbelow(True)\n",
    "plt.yticks(np.arange(0, np.max(n) + 1, 1))\n",
    "\n",
    "# Display the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisiere Ergebnisse im Raum\n",
    "Die Suchresultate werden als Punkte auf dem \"Loeffel\"-Plan dargestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the coordinates of the search results.\n",
    "def transform_coords(x, y):\n",
    "    transformer = Transformer.from_crs(2056, 4326)\n",
    "    return transformer.transform(x, y)\n",
    "results['lat'], results['lon'] = zip(*results.apply(\n",
    "    lambda row: transform_coords(row['coord_x'], row['coord_y']), axis=1\n",
    "    ))\n",
    "\n",
    "# Define the basemap.\n",
    "wms = WMSLayer(\n",
    "    url='https://wms.geo.bs.ch/',\n",
    "    layers='HP_Situationsplan_Basel_1862',\n",
    "    attribution='Geodaten Kanton Basel-Stadt'\n",
    ")\n",
    "m = Map(basemap=wms, center=(47.557, 7.595), zoom=14)\n",
    "\n",
    "# Add the search results to the map.\n",
    "for index, row in results.iterrows():\n",
    "    \n",
    "    # Create Circle.\n",
    "    circle = Circle(location=(row['lat'], row['lon']),\n",
    "                    radius=3,\n",
    "                    color='blue',\n",
    "                    fill_color='blue')\n",
    "    \n",
    "    # Add pop-up to display attributes of the circle.\n",
    "    popup_content = f\"\"\"ID: {row['id']}<br>\n",
    "    House: {row['house']}<br>\n",
    "    Pages: {row['pages']}<br>\n",
    "    Year: {row['year']}\"\"\"\n",
    "    popup = Popup(location=(row['lat'], row['lon']), child=widgets.HTML(popup_content), close_button=True)\n",
    "    circle.popup = popup\n",
    "\n",
    "    # Add the circle to the map.\n",
    "    m.add_layer(circle)\n",
    "\n",
    "#  Display the map.\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suchresultate exportieren\n",
    "Exportiere Suchergebnisse als Exceltabelle in Arbeitsverzeichnis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filename for exported excel table.\n",
    "FILENAME_EXCEL = 'search_results.xlsx'\n",
    "\n",
    "# Export all search results as excel file.\n",
    "if in_colab:\n",
    "    results.to_excel(WORKING_DIRECTORY + FILENAME_EXCEL, index=False)\n",
    "else:\n",
    "    results.to_excel(FILENAME_EXCEL, index=False)\n",
    "\n",
    "print('Search results were exported.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
